# PyTorchåŸºç¡€å®æˆ˜

## 1. PyTorchæ ¸å¿ƒæ¦‚å¿µ

### 1.1 PyTorchè®¾è®¡å“²å­¦

**PyTorch** æ˜¯ä¸€ä¸ªåŸºäºTorchçš„Pythonå¼€æºæœºå™¨å­¦ä¹ åº“ï¼Œç”±Facebookçš„AIç ”ç©¶å›¢é˜Ÿå¼€å‘ã€‚å…¶æ ¸å¿ƒè®¾è®¡å“²å­¦æ˜¯ï¼š

- **åŠ¨æ€è®¡ç®—å›¾ï¼ˆDynamic Computational Graphï¼‰**ï¼šè¿è¡Œæ—¶æ„å»ºè®¡ç®—å›¾ï¼Œçµæ´»æ€§å¼º
- **Pythonicé£æ ¼**ï¼šä¸Pythonç”Ÿæ€æ— ç¼é›†æˆï¼Œæ˜“äºå­¦ä¹ å’Œä½¿ç”¨
- **GPUåŠ é€Ÿ**ï¼šåŸç”Ÿæ”¯æŒCUDAï¼Œé«˜æ•ˆåˆ©ç”¨GPUè®¡ç®—èµ„æº
- **è‡ªåŠ¨æ±‚å¯¼**ï¼šå†…ç½®è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿï¼Œç®€åŒ–æ¢¯åº¦è®¡ç®—

**å¤§ç™½è¯è§£é‡Šï¼š** PyTorchå°±åƒä¹é«˜ç§¯æœ¨ï¼Œä½ å¯ä»¥è¾¹æ­è¾¹æƒ³ï¼Œéšæ—¶è°ƒæ•´ç»“æ„ï¼Œè€Œä¸éœ€è¦åƒTensorFlowé‚£æ ·å…ˆç”»å¥½å®Œæ•´çš„å›¾çº¸å†æ­å»ºã€‚

### 1.2 PyTorchæ ¸å¿ƒç»„ä»¶

```mermaid
graph TD
    A[PyTorchæ ¸å¿ƒç»„ä»¶] --> B[Tensorså¼ é‡]
    A --> C[Autogradè‡ªåŠ¨æ±‚å¯¼]
    A --> D[nn.Moduleç¥ç»ç½‘ç»œ]
    A --> E[Optimizersä¼˜åŒ–å™¨]
    A --> F[DataLoaderæ•°æ®åŠ è½½]
    
    B --> B1[CPU/GPUå¼ é‡]
    B --> B2[å¼ é‡è¿ç®—]
    
    C --> C1[è®¡ç®—å›¾æ„å»º]
    C --> C2[æ¢¯åº¦è®¡ç®—]
    
    D --> D1[å±‚å®šä¹‰]
    D --> D2[å‰å‘ä¼ æ’­]
    
    E --> E1[SGD/Adamç­‰]
    E --> E2[å‚æ•°æ›´æ–°]
    
    F --> F1[æ•°æ®é›†å°è£…]
    F --> F2[æ‰¹é‡å¤„ç†]
```

## 2. å¼ é‡æ“ä½œåŸºç¡€

### 2.1 å¼ é‡åˆ›å»ºä¸åŸºæœ¬æ“ä½œ

#### å¼ é‡åˆ›å»ºæ–¹æ³•
```python
import torch
import numpy as np

def tensor_creation_demo():
    """å¼ é‡åˆ›å»ºæ–¹æ³•æ¼”ç¤º"""
    
    print("=== å¼ é‡åˆ›å»ºæ–¹æ³• ===")
    
    # 1. ä»Pythonåˆ—è¡¨åˆ›å»º
    tensor1 = torch.tensor([1, 2, 3, 4])
    print(f"ä»åˆ—è¡¨åˆ›å»º: {tensor1}")
    
    # 2. ä»NumPyæ•°ç»„åˆ›å»º
    np_array = np.array([5, 6, 7, 8])
    tensor2 = torch.from_numpy(np_array)
    print(f"ä»NumPyåˆ›å»º: {tensor2}")
    
    # 3. ç‰¹æ®Šå¼ é‡åˆ›å»º
    zeros_tensor = torch.zeros(2, 3)        # å…¨é›¶å¼ é‡
    ones_tensor = torch.ones(3, 2)          # å…¨ä¸€å¼ é‡
    rand_tensor = torch.rand(2, 2)          # å‡åŒ€åˆ†å¸ƒéšæœºæ•°
    randn_tensor = torch.randn(2, 2)        # æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°
    
    print(f"å…¨é›¶å¼ é‡:\n{zeros_tensor}")
    print(f"å…¨ä¸€å¼ é‡:\n{ones_tensor}")
    print(f"å‡åŒ€éšæœºå¼ é‡:\n{rand_tensor}")
    print(f"æ­£æ€éšæœºå¼ é‡:\n{randn_tensor}")
    
    # 4. ç±»ä¼¼å½¢çŠ¶åˆ›å»º
    similar_tensor = torch.randn_like(zeros_tensor)  # ä¸zeros_tensorå½¢çŠ¶ç›¸åŒçš„éšæœºå¼ é‡
    print(f"ç±»ä¼¼å½¢çŠ¶åˆ›å»º:\n{similar_tensor}")

# æ‰§è¡Œå¼ é‡åˆ›å»ºæ¼”ç¤º
tensor_creation_demo()
```

#### å¼ é‡å±æ€§ä¸æ–¹æ³•
```python
def tensor_properties_demo():
    """å¼ é‡å±æ€§ä¸æ–¹æ³•æ¼”ç¤º"""
    
    # åˆ›å»ºç¤ºä¾‹å¼ é‡
    tensor = torch.randn(3, 4, 5)  # 3ç»´å¼ é‡: 3ä¸ª4x5çš„çŸ©é˜µ
    
    print("=== å¼ é‡å±æ€§ ===")
    print(f"å¼ é‡å½¢çŠ¶: {tensor.shape}")
    print(f"å¼ é‡ç»´åº¦: {tensor.dim()}")
    print(f"å¼ é‡å¤§å°: {tensor.size()}")
    print(f"æ•°æ®ç±»å‹: {tensor.dtype}")
    print(f"è®¾å¤‡ä½ç½®: {tensor.device}")
    print(f"æ˜¯å¦è¦æ±‚æ¢¯åº¦: {tensor.requires_grad}")
    
    # å¼ é‡å˜å½¢æ“ä½œ
    print("\n=== å¼ é‡å˜å½¢ ===")
    reshaped = tensor.reshape(2, 6, 5)      # æ”¹å˜å½¢çŠ¶
    flattened = tensor.flatten()            # å±•å¹³ä¸ºä¸€ç»´
    transposed = tensor.transpose(0, 1)     # è½¬ç½®ç»´åº¦
    squeezed = tensor.squeeze()             # å»é™¤å¤§å°ä¸º1çš„ç»´åº¦
    
    print(f"å˜å½¢åå½¢çŠ¶: {reshaped.shape}")
    print(f"å±•å¹³åå½¢çŠ¶: {flattened.shape}")
    print(f"è½¬ç½®åå½¢çŠ¶: {transposed.shape}")
    print(f"å‹ç¼©åå½¢çŠ¶: {squeezed.shape}")
    
    # æ•°å­¦è¿ç®—
    print("\n=== æ•°å­¦è¿ç®— ===")
    a = torch.tensor([1.0, 2.0, 3.0])
    b = torch.tensor([4.0, 5.0, 6.0])
    
    print(f"åŠ æ³•: {a + b}")
    print(f"å‡æ³•: {a - b}")
    print(f"ä¹˜æ³•: {a * b}")
    print(f"é™¤æ³•: {a / b}")
    print(f"çŸ©é˜µä¹˜æ³•: {torch.matmul(a.unsqueeze(0), b.unsqueeze(1))}")

# æ‰§è¡Œå¼ é‡å±æ€§æ¼”ç¤º
tensor_properties_demo()
```

### 2.2 GPUåŠ é€Ÿä¸è®¾å¤‡ç®¡ç†

```python
def gpu_operations_demo():
    """GPUæ“ä½œæ¼”ç¤º"""
    
    print("=== GPUè®¾å¤‡æ£€æµ‹ ===")
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… CUDAå¯ç”¨ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        print(f"GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    
    # åˆ›å»ºå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    cpu_tensor = torch.randn(1000, 1000)
    gpu_tensor = cpu_tensor.to(device)  # ç§»åŠ¨åˆ°GPU
    
    print(f"CPUå¼ é‡è®¾å¤‡: {cpu_tensor.device}")
    print(f"GPUå¼ é‡è®¾å¤‡: {gpu_tensor.device}")
    
    # æ€§èƒ½å¯¹æ¯”
    import time
    
    def benchmark_operation(device_name, tensor):
        """åŸºå‡†æµ‹è¯•å‡½æ•°"""
        start_time = time.time()
        
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼ˆè®¡ç®—å¯†é›†å‹æ“ä½œï¼‰
        result = torch.matmul(tensor, tensor.T)
        
        end_time = time.time()
        elapsed = end_time - start_time
        print(f"{device_name} è®¡ç®—æ—¶é—´: {elapsed:.4f}ç§’")
        return elapsed
    
    # CPUæ€§èƒ½æµ‹è¯•
    cpu_time = benchmark_operation("CPU", cpu_tensor)
    
    # GPUæ€§èƒ½æµ‹è¯•ï¼ˆå¦‚æœæœ‰GPUï¼‰
    if torch.cuda.is_available():
        gpu_time = benchmark_operation("GPU", gpu_tensor)
        speedup = cpu_time / gpu_time
        print(f"GPUåŠ é€Ÿæ¯”: {speedup:.2f}x")

# æ‰§è¡ŒGPUæ“ä½œæ¼”ç¤º
gpu_operations_demo()
```

## 3. è‡ªåŠ¨æ±‚å¯¼ç³»ç»Ÿ

### 3.1 Autogradæœºåˆ¶è¯¦è§£

**Autograd** æ˜¯PyTorchçš„è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è®¡ç®—å¼ é‡è¿ç®—çš„æ¢¯åº¦ã€‚

#### åŸºæœ¬Autogradæ“ä½œ
```python
def autograd_basics_demo():
    """AutogradåŸºç¡€æ¼”ç¤º"""
    
    print("=== AutogradåŸºç¡€ ===")
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
    x = torch.tensor(2.0, requires_grad=True)
    w = torch.tensor(3.0, requires_grad=True)
    b = torch.tensor(1.0, requires_grad=True)
    
    print(f"è¾“å…¥å¼ é‡: x={x}, w={w}, b={b}")
    print(f"æ˜¯å¦éœ€è¦æ¢¯åº¦: x={x.requires_grad}, w={w.requires_grad}, b={b.requires_grad}")
    
    # å‰å‘ä¼ æ’­è®¡ç®—
    y = w * x + b  # y = 3*2 + 1 = 7
    print(f"å‰å‘ä¼ æ’­ç»“æœ: y = {y}")
    
    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    y.backward()  # è®¡ç®—dy/dx, dy/dw, dy/db
    
    print("\næ¢¯åº¦è®¡ç®—ç»“æœ:")
    print(f"dy/dx = {x.grad}")  # åº”è¯¥ä¸º3.0 (dy/dx = w)
    print(f"dy/dw = {w.grad}")  # åº”è¯¥ä¸º2.0 (dy/dw = x)
    print(f"dy/db = {b.grad}")  # åº”è¯¥ä¸º1.0 (dy/db = 1)
    
    # éªŒè¯æ¢¯åº¦æ­£ç¡®æ€§
    assert torch.allclose(x.grad, torch.tensor(3.0))
    assert torch.allclose(w.grad, torch.tensor(2.0))
    assert torch.allclose(b.grad, torch.tensor(1.0))
    print("âœ… æ¢¯åº¦è®¡ç®—æ­£ç¡®")

# æ‰§è¡ŒAutogradåŸºç¡€æ¼”ç¤º
autograd_basics_demo()
```

#### å¤æ‚è®¡ç®—å›¾çš„æ¢¯åº¦è®¡ç®—
```python
def complex_autograd_demo():
    """å¤æ‚è®¡ç®—å›¾çš„Autogradæ¼”ç¤º"""
    
    print("=== å¤æ‚è®¡ç®—å›¾Autograd ===")
    
    # åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    
    # å¤æ‚çš„å‰å‘ä¼ æ’­è®¡ç®—
    y = x ** 2 + 2 * x + 1          # äºŒæ¬¡å‡½æ•°
    z = torch.sin(y) + torch.log(y)  # ä¸‰è§’å‡½æ•° + å¯¹æ•°
    w = z.sum()                      # æ±‚å’Œ
    
    print(f"è¾“å…¥: x = {x}")
    print(f"ä¸­é—´ç»“æœ: y = {y}")
    print(f"ä¸­é—´ç»“æœ: z = {z}")
    print(f"æœ€ç»ˆç»“æœ: w = {w}")
    
    # åå‘ä¼ æ’­
    w.backward()
    
    print(f"\næ¢¯åº¦: dw/dx = {x.grad}")
    
    # æ‰‹åŠ¨éªŒè¯æ¢¯åº¦ï¼ˆä½¿ç”¨é“¾å¼æ³•åˆ™ï¼‰
    manual_grad = torch.zeros_like(x)
    for i in range(len(x)):
        # dy/dx = 2x + 2
        dy_dx = 2 * x[i] + 2
        # dz/dy = cos(y) + 1/y
        dz_dy = torch.cos(y[i]) + 1 / y[i]
        # dw/dz = 1 (å› ä¸ºwæ˜¯zçš„å’Œ)
        dw_dz = 1
        # é“¾å¼æ³•åˆ™: dw/dx = dw/dz * dz/dy * dy/dx
        manual_grad[i] = dw_dz * dz_dy * dy_dx
    
    print(f"æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦: {manual_grad}")
    print(f"æ¢¯åº¦ä¸€è‡´æ€§: {torch.allclose(x.grad, manual_grad)}")

# æ‰§è¡Œå¤æ‚Autogradæ¼”ç¤º
complex_autograd_demo()
```

### 3.2 æ¢¯åº¦æ§åˆ¶ä¸å†…å­˜ç®¡ç†

#### æ¢¯åº¦æ§åˆ¶æ–¹æ³•
```python
def gradient_control_demo():
    """æ¢¯åº¦æ§åˆ¶æ¼”ç¤º"""
    
    print("=== æ¢¯åº¦æ§åˆ¶ ===")
    
    # 1. ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆæ¨ç†é˜¶æ®µï¼‰
    x = torch.tensor([1.0, 2.0], requires_grad=True)
    
    with torch.no_grad():  # åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡å†…ä¸è®¡ç®—æ¢¯åº¦
        y = x * 2
        print(f"no_gradæ¨¡å¼: y.requires_grad = {y.requires_grad}")
    
    # 2. æ‰‹åŠ¨è®¾ç½®requires_grad
    x.requires_grad_(False)  # ç¦ç”¨æ¢¯åº¦
    print(f"æ‰‹åŠ¨ç¦ç”¨æ¢¯åº¦: x.requires_grad = {x.requires_grad}")
    
    # 3. åˆ†ç¦»å¼ é‡ï¼ˆdetachï¼‰
    x = torch.tensor([1.0, 2.0], requires_grad=True)
    y = x * 2
    z = y.detach()  # åˆ†ç¦»yï¼Œzä¸å‚ä¸æ¢¯åº¦è®¡ç®—
    w = z * 3
    
    w.backward(torch.ones_like(w))
    print(f"åˆ†ç¦»åæ¢¯åº¦: x.grad = {x.grad}")  # åªæœ‰yå‚ä¸æ¢¯åº¦è®¡ç®—
    
    # 4. æ¢¯åº¦æ¸…é›¶ï¼ˆè®­ç»ƒå¾ªç¯ä¸­é‡è¦ï¼‰
    x = torch.tensor(1.0, requires_grad=True)
    
    # ç¬¬ä¸€æ¬¡åå‘ä¼ æ’­
    y1 = x ** 2
    y1.backward()
    print(f"ç¬¬ä¸€æ¬¡æ¢¯åº¦: {x.grad}")
    
    # ä¸æ¸…é›¶ç›´æ¥ç¬¬äºŒæ¬¡åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ä¼šç´¯ç§¯ï¼‰
    y2 = x ** 3
    y2.backward()
    print(f"ç´¯ç§¯æ¢¯åº¦: {x.grad}")
    
    # æ¸…é›¶åé‡æ–°è®¡ç®—
    x.grad.zero_()
    y3 = x ** 4
    y3.backward()
    print(f"æ¸…é›¶åæ¢¯åº¦: {x.grad}")

# æ‰§è¡Œæ¢¯åº¦æ§åˆ¶æ¼”ç¤º
gradient_control_demo()
```

## 4. ç¥ç»ç½‘ç»œæ¨¡å—å®æˆ˜

### 4.1 è‡ªå®šä¹‰ç¥ç»ç½‘ç»œå±‚

#### åŸºç¡€å±‚å®ç°
```python
import torch.nn as nn
import torch.nn.functional as F

class CustomLinear(nn.Module):
    """
    è‡ªå®šä¹‰çº¿æ€§å±‚å®ç°
    
    åŠŸèƒ½: å®ç° y = xW^T + b
    å‚æ•°: in_features(è¾“å…¥ç‰¹å¾æ•°), out_features(è¾“å‡ºç‰¹å¾æ•°)
    """
    def __init__(self, in_features, out_features):
        super(CustomLinear, self).__init__()
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.bias = nn.Parameter(torch.randn(out_features))
        
        # Xavieråˆå§‹åŒ–ï¼ˆæ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ï¼‰
        nn.init.xavier_uniform_(self.weight)
        nn.init.zeros_(self.bias)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­è®¡ç®—"""
        # x: (batch_size, in_features)
        # weight: (out_features, in_features)
        # bias: (out_features)
        
        # çŸ©é˜µä¹˜æ³•: x * W^T + b
        output = torch.matmul(x, self.weight.T) + self.bias
        return output

class CustomReLU(nn.Module):
    """è‡ªå®šä¹‰ReLUæ¿€æ´»å±‚"""
    def __init__(self):
        super(CustomReLU, self).__init__()
    
    def forward(self, x):
        return torch.maximum(torch.tensor(0.0), x)  # ReLU: max(0, x)

# æµ‹è¯•è‡ªå®šä¹‰å±‚
def test_custom_layers():
    """æµ‹è¯•è‡ªå®šä¹‰å±‚åŠŸèƒ½"""
    
    print("=== è‡ªå®šä¹‰å±‚æµ‹è¯• ===")
    
    # åˆ›å»ºè‡ªå®šä¹‰å±‚å®ä¾‹
    linear_layer = CustomLinear(in_features=10, out_features=5)
    relu_layer = CustomReLU()
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 4
    x = torch.randn(batch_size, 10)
    
    # å‰å‘ä¼ æ’­
    linear_output = linear_layer(x)
    relu_output = relu_layer(linear_output)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"çº¿æ€§å±‚è¾“å‡ºå½¢çŠ¶: {linear_output.shape}")
    print(f"ReLUå±‚è¾“å‡ºå½¢çŠ¶: {relu_output.shape}")
    print(f"ReLUè¾“å‡ºèŒƒå›´: [{relu_output.min():.3f}, {relu_output.max():.3f}]")
    
    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in linear_layer.parameters())
    print(f"çº¿æ€§å±‚å‚æ•°æ•°é‡: {total_params}")
    print(f"æƒé‡å½¢çŠ¶: {linear_layer.weight.shape}")
    print(f"åç½®å½¢çŠ¶: {linear_layer.bias.shape}")

# æ‰§è¡Œè‡ªå®šä¹‰å±‚æµ‹è¯•
test_custom_layers()
```

#### å¤æ‚ç½‘ç»œæ¶æ„å®ç°
```python
class AdvancedCNN(nn.Module):
    """
    é«˜çº§CNNæ¶æ„å®ç°
    
    ç‰¹ç‚¹:
    - æ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰
    - æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰
    - Dropoutæ­£åˆ™åŒ–
    - è‡ªé€‚åº”æ± åŒ–
    """
    def __init__(self, num_classes=10, dropout_rate=0.3):
        super(AdvancedCNN, self).__init__()
        
        # ç‰¹å¾æå–éƒ¨åˆ†
        self.feature_extractor = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(dropout_rate),
            
            # ç¬¬äºŒä¸ªå·ç§¯å—ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(dropout_rate),
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å—
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((4, 4))  # è‡ªé€‚åº”æ± åŒ–åˆ°4x4
        )
        
        # åˆ†ç±»å™¨éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(256 * 4 * 4, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        # ç‰¹å¾æå–
        features = self.feature_extractor(x)
        
        # å±•å¹³
        features = features.view(features.size(0), -1)
        
        # åˆ†ç±»
        output = self.classifier(features)
        
        return output

# æµ‹è¯•é«˜çº§CNN
def test_advanced_cnn():
    """æµ‹è¯•é«˜çº§CNNæ¶æ„"""
    
    print("=== é«˜çº§CNNæµ‹è¯• ===")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = AdvancedCNN(num_classes=10)
    
    # æµ‹è¯•è¾“å…¥ï¼ˆæ¨¡æ‹ŸCIFAR-10å›¾åƒï¼‰
    batch_size, channels, height, width = 8, 3, 32, 32
    x = torch.randn(batch_size, channels, height, width)
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºç¤ºä¾‹: {output[0][:5]}")  # æ˜¾ç¤ºå‰5ä¸ªç±»åˆ«çš„logits
    
    # æ¨¡å‹å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    # æ¨¡å‹ç»“æ„å¯è§†åŒ–
    print("\næ¨¡å‹ç»“æ„:")
    print(model)

# æ‰§è¡Œé«˜çº§CNNæµ‹è¯•
test_advanced_cnn()
```

### 4.2 æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨

#### å¸¸ç”¨æŸå¤±å‡½æ•°å®ç°
```python
def loss_functions_demo():
    """æŸå¤±å‡½æ•°æ¼”ç¤º"""
    
    print("=== å¸¸ç”¨æŸå¤±å‡½æ•° ===")
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size, num_classes = 4, 3
    y_pred = torch.randn(batch_size, num_classes)  # æ¨¡å‹é¢„æµ‹ï¼ˆlogitsï¼‰
    y_true = torch.tensor([0, 2, 1, 0])           # çœŸå®æ ‡ç­¾
    
    print(f"é¢„æµ‹å€¼å½¢çŠ¶: {y_pred.shape}")
    print(f"çœŸå®æ ‡ç­¾: {y_true}")
    
    # 1. äº¤å‰ç†µæŸå¤±ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    criterion_ce = nn.CrossEntropyLoss()
    loss_ce = criterion_ce(y_pred, y_true)
    print(f"äº¤å‰ç†µæŸå¤±: {loss_ce:.4f}")
    
    # 2. å‡æ–¹è¯¯å·®æŸå¤±ï¼ˆå›å½’ä»»åŠ¡ï¼‰
    y_pred_reg = torch.randn(batch_size, 1)
    y_true_reg = torch.randn(batch_size, 1)
    
    criterion_mse = nn.MSELoss()
    loss_mse = criterion_mse(y_pred_reg, y_true_reg)
    print(f"å‡æ–¹è¯¯å·®æŸå¤±: {loss_mse:.4f}")
    
    # 3. äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆäºŒåˆ†ç±»ï¼‰
    y_pred_binary = torch.sigmoid(torch.randn(batch_size, 1))  # æ¦‚ç‡å€¼
    y_true_binary = torch.randint(0, 2, (batch_size, 1)).float()
    
    criterion_bce = nn.BCELoss()
    loss_bce = criterion_bce(y_pred_binary, y_true_binary)
    print(f"äºŒå…ƒäº¤å‰ç†µæŸå¤±: {loss_bce:.4f}")
    
    # 4. è‡ªå®šä¹‰æŸå¤±å‡½æ•°
    class FocalLoss(nn.Module):
        """Focal Lossï¼ˆç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰"""
        def __init__(self, alpha=1, gamma=2):
            super(FocalLoss, self).__init__()
            self.alpha = alpha
            self.gamma = gamma
        
        def forward(self, inputs, targets):
            # è®¡ç®—äº¤å‰ç†µ
            ce_loss = F.cross_entropy(inputs, targets, reduction='none')
            
            # è®¡ç®—æ¦‚ç‡
            pt = torch.exp(-ce_loss)
            
            # Focal Losså…¬å¼
            focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
            
            return focal_loss.mean()
    
    # æµ‹è¯•Focal Loss
    focal_criterion = FocalLoss()
    loss_focal = focal_criterion(y_pred, y_true)
    print(f"Focal Loss: {loss_focal:.4f}")

# æ‰§è¡ŒæŸå¤±å‡½æ•°æ¼”ç¤º
loss_functions_demo()
```

#### ä¼˜åŒ–å™¨é…ç½®ä¸ä½¿ç”¨
```python
def optimizers_demo():
    """ä¼˜åŒ–å™¨æ¼”ç¤º"""
    
    print("=== ä¼˜åŒ–å™¨é…ç½® ===")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 5),
        nn.ReLU(),
        nn.Linear(5, 1)
    )
    
    # 1. SGDä¼˜åŒ–å™¨
    optimizer_sgd = torch.optim.SGD(
        model.parameters(),
        lr=0.01,           # å­¦ä¹ ç‡
        momentum=0.9,      # åŠ¨é‡
        weight_decay=1e-4  # L2æ­£åˆ™åŒ–
    )
    
    # 2. Adamä¼˜åŒ–å™¨
    optimizer_adam = torch.optim.Adam(
        model.parameters(),
        lr=0.001,          # å­¦ä¹ ç‡
        betas=(0.9, 0.999), # åŠ¨é‡å‚æ•°
        weight_decay=1e-4  # L2æ­£åˆ™åŒ–
    )
    
    # 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer_adam,
        step_size=10,      # æ¯10ä¸ªepochè°ƒæ•´ä¸€æ¬¡
        gamma=0.1          # å­¦ä¹ ç‡ä¹˜ä»¥0.1
    )
    
    print("ä¼˜åŒ–å™¨é…ç½®å®Œæˆ")
    print(f"SGDå‚æ•°ç»„æ•°: {len(optimizer_sgd.param_groups)}")
    print(f"Adamå‚æ•°ç»„æ•°: {len(optimizer_adam.param_groups)}")
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    def simulate_training():
        """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹"""
        
        # æ¨¡æ‹Ÿæ•°æ®
        x = torch.randn(100, 10)
        y = torch.randn(100, 1)
        
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨è®­ç»ƒ
        model.train()
        
        for epoch in range(5):
            # å‰å‘ä¼ æ’­
            y_pred = model(x)
            loss = F.mse_loss(y_pred, y)
            
            # åå‘ä¼ æ’­
            optimizer_adam.zero_grad()  # æ¢¯åº¦æ¸…é›¶
            loss.backward()             # è®¡ç®—æ¢¯åº¦
            optimizer_adam.step()       # æ›´æ–°å‚æ•°
            
            # å­¦ä¹ ç‡è°ƒæ•´
            scheduler.step()
            
            current_lr = scheduler.get_last_lr()[0]
            print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}, LR = {current_lr:.6f}")
    
    # æ‰§è¡Œæ¨¡æ‹Ÿè®­ç»ƒ
    simulate_training()

# æ‰§è¡Œä¼˜åŒ–å™¨æ¼”ç¤º
optimizers_demo()
```

## 5. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†

### 5.1 è‡ªå®šä¹‰æ•°æ®é›†ç±»

```python
from torch.utils.data import Dataset, DataLoader
import os
from PIL import Image

class CustomImageDataset(Dataset):
    """
    è‡ªå®šä¹‰å›¾åƒæ•°æ®é›†ç±»
    
    åŠŸèƒ½: åŠ è½½å›¾åƒæ•°æ®ï¼Œæ”¯æŒæ•°æ®å¢å¼ºå’Œé¢„å¤„ç†
    """
    def __init__(self, image_dir, label_file, transform=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        å‚æ•°:
            image_dir: å›¾åƒç›®å½•è·¯å¾„
            label_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            transform: æ•°æ®å¢å¼ºå˜æ¢
        """
        self.image_dir = image_dir
        self.transform = transform
        
        # åŠ è½½æ ‡ç­¾æ•°æ®
        self.image_paths = []
        self.labels = []
        
        with open(label_file, 'r') as f:
            for line in f:
                image_name, label = line.strip().split(',')
                self.image_paths.append(os.path.join(image_dir, image_name))
                self.labels.append(int(label))
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        # åŠ è½½å›¾åƒ
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')  # ç¡®ä¿RGBæ ¼å¼
        
        # è·å–æ ‡ç­¾
        label = self.labels[idx]
        
        # æ•°æ®å¢å¼º/é¢„å¤„ç†
        if self.transform:
            image = self.transform(image)
        
        return image, label

# æ•°æ®å¢å¼ºå˜æ¢
from torchvision import transforms

def create_transforms():
    """åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•çš„æ•°æ®å˜æ¢"""
    
    # è®­ç»ƒæ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(224),      # éšæœºè£å‰ªå¹¶è°ƒæ•´å¤§å°
        transforms.RandomHorizontalFlip(0.5),   # éšæœºæ°´å¹³ç¿»è½¬
        transforms.RandomRotation(10),          # éšæœºæ—‹è½¬
        transforms.ColorJitter(0.2, 0.2, 0.2), # é¢œè‰²æŠ–åŠ¨
        transforms.ToTensor(),                  # è½¬æ¢ä¸ºå¼ é‡
        transforms.Normalize(                   # æ ‡å‡†åŒ–
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    # æµ‹è¯•æ•°æ®å˜æ¢ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    test_transform = transforms.Compose([
        transforms.Resize(256),                 # è°ƒæ•´å¤§å°
        transforms.CenterCrop(224),             # ä¸­å¿ƒè£å‰ª
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    return train_transform, test_transform

# æ•°æ®åŠ è½½å™¨ä½¿ç”¨ç¤ºä¾‹
def dataloader_demo():
    """æ•°æ®åŠ è½½å™¨æ¼”ç¤º"""
    
    print("=== æ•°æ®åŠ è½½å™¨æ¼”ç¤º ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®è·¯å¾„ï¼‰
    class MockDataset(Dataset):
        def __init__(self, size=100):
            self.data = torch.randn(size, 3, 32, 32)
            self.labels = torch.randint(0, 10, (size,))
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx], self.labels[idx]
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = MockDataset(100)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=16,          # æ‰¹é‡å¤§å°
        shuffle=True,           # æ˜¯å¦æ‰“ä¹±æ•°æ®
        num_workers=2,          # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
        pin_memory=True         # é”é¡µå†…å­˜ï¼ˆåŠ é€ŸGPUä¼ è¾“ï¼‰
    )
    
    # éå†æ•°æ®åŠ è½½å™¨
    for batch_idx, (data, labels) in enumerate(dataloader):
        print(f"æ‰¹æ¬¡ {batch_idx + 1}:")
        print(f"  æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print(f"  æ ‡ç­¾ç¤ºä¾‹: {labels[:5]}")
        
        if batch_idx == 2:  # åªæ˜¾ç¤ºå‰3ä¸ªæ‰¹æ¬¡
            break

# æ‰§è¡Œæ•°æ®åŠ è½½å™¨æ¼”ç¤º
dataloader_demo()
```

## 6. å®Œæ•´è®­ç»ƒæµç¨‹å®æˆ˜

### 6.1 æ¨¡å‹è®­ç»ƒæ¨¡æ¿

```python
def complete_training_pipeline():
    """å®Œæ•´è®­ç»ƒæµç¨‹æ¼”ç¤º"""
    
    print("=== å®Œæ•´è®­ç»ƒæµç¨‹ ===")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. æ•°æ®å‡†å¤‡ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
    class SimpleDataset(Dataset):
        def __init__(self, size=1000):
            self.data = torch.randn(size, 10)
            self.labels = (self.data.sum(dim=1) > 0).long()  # ç®€å•äºŒåˆ†ç±»
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            return self.data[idx], self.labels[idx]
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    dataset = SimpleDataset(1000)
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_size, test_size]
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 2. æ¨¡å‹å®šä¹‰
    class SimpleModel(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(SimpleModel, self).__init__()
            self.network = nn.Sequential(
                nn.Linear(input_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_size, output_size)
            )
        
        def forward(self, x):
            return self.network(x)
    
    model = SimpleModel(input_size=10, hidden_size=64, output_size=2)
    model = model.to(device)
    
    # 3. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # 4. è®­ç»ƒå¾ªç¯
    num_epochs = 10
    train_losses = []
    test_accuracies = []
    
    for epoch in range(num_epochs):
        # è®­ç»ƒæ¨¡å¼
        model.train()
        epoch_loss = 0.0
        
        for batch_idx, (data, labels) in enumerate(train_loader):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            data, labels = data.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(data)
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        # è¯„ä¼°æ¨¡å¼
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(device), labels.to(device)
                outputs = model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        test_accuracies.append(accuracy)
        
        print(f"Epoch [{epoch+1}/{num_epochs}], "
              f"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
    
    # 5. ç»“æœå¯è§†åŒ–
    plt.figure(figsize=(12, 4))
    
    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, 'b-', label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()
    plt.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(test_accuracies, 'r-', label='Test Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title('Test Accuracy')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    print("âœ… è®­ç»ƒå®Œæˆ!")

# æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
complete_training_pipeline()
```

## 7. æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### 7.1 æ¨¡å‹æŒä¹…åŒ–æ–¹æ³•

```python
def model_persistence_demo():
    """æ¨¡å‹ä¿å­˜ä¸åŠ è½½æ¼”ç¤º"""
    
    print("=== æ¨¡å‹æŒä¹…åŒ– ===")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 5),
        nn.ReLU(),
        nn.Linear(5, 2)
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒï¼ˆæ›´æ–°å‚æ•°ï¼‰
    optimizer = torch.optim.Adam(model.parameters())
    for _ in range(10):
        x = torch.randn(32, 10)
        y = torch.randint(0, 2, (32,))
        
        optimizer.zero_grad()
        output = model(x)
        loss = F.cross_entropy(output, y)
        loss.backward()
        optimizer.step()
    
    # 1. ä¿å­˜æ•´ä¸ªæ¨¡å‹
    torch.save(model, 'complete_model.pth')
    print("âœ… å®Œæ•´æ¨¡å‹ä¿å­˜æˆåŠŸ")
    
    # åŠ è½½æ•´ä¸ªæ¨¡å‹
    loaded_model = torch.load('complete_model.pth')
    print("âœ… å®Œæ•´æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 2. ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆæ¨èï¼‰
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': 10,
        'loss': loss.item()
    }, 'checkpoint.pth')
    print("âœ… æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load('checkpoint.pth')
    
    # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½çŠ¶æ€
    new_model = nn.Sequential(
        nn.Linear(10, 5),
        nn.ReLU(),
        nn.Linear(5, 2)
    )
    new_model.load_state_dict(checkpoint['model_state_dict'])
    
    print("âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
    print(f"è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
    print(f"æœ€åæŸå¤±: {checkpoint['loss']:.4f}")
    
    # 3. æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼ˆç”¨äºéƒ¨ç½²ï¼‰
    def export_to_onnx():
        """å¯¼å‡ºä¸ºONNXæ ¼å¼"""
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        dummy_input = torch.randn(1, 10)
        
        # å¯¼å‡ºæ¨¡å‹
        torch.onnx.export(
            model,
            dummy_input,
            'model.onnx',
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        print("âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ")
    
    # æ‰§è¡ŒONNXå¯¼å‡º
    export_to_onnx()
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import os
    for file in ['complete_model.pth', 'checkpoint.pth', 'model.onnx']:
        if os.path.exists(file):
            os.remove(file)
            print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file}")

# æ‰§è¡Œæ¨¡å‹æŒä¹…åŒ–æ¼”ç¤º
model_persistence_demo()
```

## 8. ä¼ä¸šçº§æœ€ä½³å®è·µ

### 8.1 ä»£ç ç»„ç»‡ä¸æ¨¡å—åŒ–

#### é¡¹ç›®ç»“æ„å»ºè®®
```
project/
â”œâ”€â”€ models/           # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cnn.py       # CNNæ¨¡å‹
â”‚   â”œâ”€â”€ rnn.py       # RNNæ¨¡å‹
â”‚   â””â”€â”€ transformer.py
â”œâ”€â”€ data/            # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ datasets.py  # æ•°æ®é›†ç±»
â”‚   â””â”€â”€ transforms.py # æ•°æ®å¢å¼º
â”œâ”€â”€ utils/           # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py   # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ config/          # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ train.py         # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py      # è¯„ä¼°è„šæœ¬
â””â”€â”€ requirements.txt
```

### 8.2 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

#### å†…å­˜ä¼˜åŒ–
```python
def memory_optimization_tips():
    """å†…å­˜ä¼˜åŒ–æŠ€å·§"""
    
    print("=== å†…å­˜ä¼˜åŒ–æŠ€å·§ ===")
    
    # 1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    from torch.cuda.amp import autocast, GradScaler
    
    scaler = GradScaler()
    
    def mixed_precision_training():
        """æ··åˆç²¾åº¦è®­ç»ƒç¤ºä¾‹"""
        model = nn.Linear(10, 1).cuda()
        optimizer = torch.optim.Adam(model.parameters())
        
        for epoch in range(5):
            x = torch.randn(32, 10).cuda()
            y = torch.randn(32, 1).cuda()
            
            optimizer.zero_grad()
            
            # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            with autocast():
                output = model(x)
                loss = F.mse_loss(output, y)
            
            # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        
        print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå®Œæˆ")
    
    # 2. æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batch sizeï¼‰
    def gradient_accumulation():
        """æ¢¯åº¦ç´¯ç§¯ç¤ºä¾‹"""
        model = nn.Linear(10, 1)
        optimizer = torch.optim.Adam(model.parameters())
        
        accumulation_steps = 4  # ç´¯ç§¯4ä¸ªbatch
        
        for i, (x, y) in enumerate(dataloader):
            output = model(x)
            loss = F.mse_loss(output, y)
            
            # ç¼©æ”¾æŸå¤±ï¼ˆé™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼‰
            loss = loss / accumulation_steps
            loss.backward()
            
            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
        
        print("âœ… æ¢¯åº¦ç´¯ç§¯è®­ç»ƒå®Œæˆ")
    
    # 3. æ¨¡å‹å‰ªæï¼ˆå‡å°‘å‚æ•°ï¼‰
    def model_pruning():
        """æ¨¡å‹å‰ªæç¤ºä¾‹"""
        model = nn.Sequential(
            nn.Linear(100, 50),
            nn.ReLU(),
            nn.Linear(50, 10)
        )
        
        # éšæœºå‰ªæï¼ˆå°†20%çš„æƒé‡è®¾ä¸º0ï¼‰
        for module in model.modules():
            if isinstance(module, nn.Linear):
                prune.random_unstructured(module, name="weight", amount=0.2)
        
        print("âœ… æ¨¡å‹å‰ªæå®Œæˆ")
    
    print("å†…å­˜ä¼˜åŒ–æŠ€å·§æ¼”ç¤ºå®Œæˆ")

# æ‰§è¡Œå†…å­˜ä¼˜åŒ–æ¼”ç¤º
memory_optimization_tips()
```

---

**å‚è€ƒèµ„æ–™ï¼š**
[^1]: [PyTorchå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/index.html)
[^2]: [PyTorchæ•™ç¨‹](https://pytorch.org/tutorials/)
[^3]: [ã€Šæ·±åº¦å­¦ä¹ æ¡†æ¶PyTorchï¼šå…¥é—¨ä¸å®è·µã€‹](https://book.douban.com/subject/27665114/)