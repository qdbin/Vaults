# Transformer框架模块

## 模块概述

本模块深入讲解Transformer架构原理、注意力机制以及HuggingFace Transformers库的使用，为大模型开发和应用提供核心技术支持。

## 知识卡片列表

### 基础理论
- [Transformer架构原理](Transformer架构原理.md)
- [注意力机制详解](注意力机制详解.md)
- [位置编码机制](位置编码机制.md)
- [自注意力与交叉注意力](自注意力与交叉注意力.md)

### Transformers库
- [HuggingFace Transformers库](HuggingFace-Transformers库.md)
- [预训练模型加载与使用](预训练模型加载与使用.md)
- [模型微调技术](模型微调技术.md)
- [文本生成与解码策略](文本生成与解码策略.md)

### 实战应用
- [文本分类实战](文本分类实战.md)
- [命名实体识别](命名实体识别.md)
- [文本生成应用](文本生成应用.md)
- [多模态Transformer](多模态Transformer.md)

### 进阶技术
- [BERT原理与应用](BERT原理与应用.md)
- [GPT系列模型](GPT系列模型.md)
- [T5模型与文本到文本](T5模型与文本到文本.md)
- [模型压缩与加速](模型压缩与加速.md)

## 学习建议

### 新手阶段（2-3周）
1. 理解Transformer基础架构
2. 掌握注意力机制原理
3. 学会使用HuggingFace基础API

### 进阶阶段（3-6周）
1. 深入理解各种预训练模型
2. 掌握模型微调技术
3. 完成文本处理实战项目

### 专家阶段（2个月以上）
1. 模型架构设计与优化
2. 多模态应用开发
3. 性能优化与部署

## 标签体系

- `#Transformer架构` - Transformer核心原理
- `#注意力机制` - 自注意力、交叉注意力
- `#HuggingFace` - Transformers库使用
- `#预训练模型` - BERT、GPT等模型
- `#文本处理` - NLP应用实战

## 快速导航

- [返回知识地图](../CardHome.md)
- [上一个模块：深度学习与PyTorch](../02-深度学习与PyTorch/README.md)
- [下一个模块：LangChain开发](../04-LangChain开发/README.md)